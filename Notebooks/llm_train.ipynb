{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ef9554",
   "metadata": {},
   "source": [
    "# Quick Overview Flow of LLM Training\n",
    "\n",
    "1. Tokenization: text is broken down to token IDs using BPR (Byte Pair Encoding)\n",
    "\n",
    "2. Token Embeddings: Token IDs are converted into embeddings each token ids is mapped to a vector with its own meaning of token\n",
    "\n",
    "3. For each transformer layer:\n",
    "    - RMSNorm\n",
    "    - QKV projection\n",
    "    - Apply RoPE to Q and K\n",
    "    - Causal self-attention (+KV cache at inference time)\n",
    "    - Output projection + residual\n",
    "    - RMSNorm\n",
    "    - MLP (SwiGLU) + residual\n",
    "4. Final hidden layer -> LM Head logits -> softmax\n",
    "\n",
    "5. Training cross-entropy vs next token -> backprop -> update weights\n",
    "\n",
    "6. Inference: sample/greedy/beam -> loop token by token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551916c",
   "metadata": {},
   "source": [
    "# Post Training\n",
    "\n",
    "## Common Modern Stack\n",
    "\n",
    "1. SFT (supervised fine tuning)\n",
    "Then Either\n",
    "- RLHF (PPO)\n",
    "- DPO (simpler preference tuning)\n",
    "- GRPO (DeepSeek-ish, no critic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decca161",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1abafb4",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "### Pick a. small instruct model, build deterministic parsing prompt + JSON Schema Validator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397c08f",
   "metadata": {},
   "source": [
    "Limited to text generation\n",
    "\n",
    "\n",
    "Libraries used:\n",
    "- PyTorch\n",
    "- Transformers\n",
    "\n",
    "Model Size:\n",
    "- 1B ~9B parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef84817",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "## Type of LM Models (Language Models)\n",
    "\n",
    "### Representation Models (Encoder-Only)\n",
    "\n",
    "Models like BERT are feature extraction models. They don't generate text but are good at understanding context and creating embeddings for tasks like classification or named-entity recognition.\n",
    "\n",
    "### Generative Models (Decoder-Only)\n",
    "\n",
    "These models are like GPT, Llama and are completion machines designed to generate text one token at a time. These are the ones you use to write or generate text.\n",
    "\n",
    "## Causal Language Models (CLM)\n",
    "\n",
    "The Goal: The model is trained to predict the next token in a sequence based only on the tokens that came before it.\n",
    "\n",
    "Autoregressive Nature: It only looks at the past and never the future using masked self-attention. It predicts a word and adds it to the prompt and then predicts the next word.\n",
    "\n",
    "## Masked Language Modeling (MLM)\n",
    "\n",
    "    - Used by encoder models.\n",
    "    - Instead of predicting the next token, it predicts a missing word in the middle of the sequence.\n",
    "    - This helps the model learna deep understanding of how the words relate to each other bidirectionally.\n",
    "\n",
    "## Sequence to Sequence / Encoder-Decoder\n",
    "\n",
    "    - This architecture uses an encoder to read the input and decoder to write the output.\n",
    "    - Useful for translation or summarization tasks.\n",
    "\n",
    "## Contrastive Learning\n",
    "\n",
    "    - Technique used ot train Embedding models.\n",
    "    - It teaches a model to recognize when two pieces of text are similar and when they are different by comparing pairs of data.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5538cf",
   "metadata": {},
   "source": [
    "## [gpt2](https://huggingface.co/openai-community/gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b590801",
   "metadata": {},
   "source": [
    "## [meta-llama/Llama-3.1-8B-Instruct (Will not use, too large)](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "\n",
    "**Type**: Instruction-tuned chat model\n",
    "8 Billion Parameters (small to medium sized)\n",
    "Instruct Model = pretrained model\n",
    "Context Window = 128,000 tokens \n",
    "\n",
    "**Architecture** = Dense Transformer\n",
    "\n",
    "Mixture Of Experts (x)\n",
    "\n",
    "State Space Models (x)\n",
    "\n",
    "Recurrent Neural Networks (x)\n",
    "\n",
    "\n",
    "**Knowledge Cutoff** = Dec 2023\n",
    "\n",
    "**Attention Mechanism** = Grouped-Query Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b0f06",
   "metadata": {},
   "source": [
    "## [meta-llama/Llama-3.2-1B-Instruct (we will be using this))](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "\n",
    "**Type**:\n",
    "- decoder only\n",
    "- auto regressive transformer\n",
    "**Key Bits:**\n",
    "- Grouped Query Attention\n",
    "- RoPE positional embedding\n",
    "- RMSNorm \n",
    "- SwiGLU MLP\n",
    "\n",
    "**Size / Core hyperparameters**\n",
    "- 32 layers\n",
    "- 4096 hidden size\n",
    "- 32 attention heads\n",
    "- 8 KV heads\n",
    "\n",
    "**Knowledge Cutoff**: Dec 2023\n",
    "\n",
    "**Context Window:** 128K tokens\n",
    "\n",
    "\n",
    "\n",
    "Aligned with SFT and RLHF for assistant style behavior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21656fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulpark/SandBox/CalendarProject/sandboxenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer, GPT2Model, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85582eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select model from huggingface\n",
    "# # model_llama3_id = \"meta-llama/Llama-3.1-8B-Instruct\" # this model is too big to run\n",
    "# model_gpt2_id = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f934b7",
   "metadata": {},
   "source": [
    "Getting specific token for model and checking if token is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b747a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get token from environment variable\n",
    "# token = os.environ.get(\"HF_TOKEN\")\n",
    "# # checking if the token works or not\n",
    "# if token:\n",
    "#     try:\n",
    "#         login(token=token)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error logging in: {e}\")\n",
    "# else:\n",
    "#     print(\"Hugging Face token not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57051fbd",
   "metadata": {},
   "source": [
    "finding specific model tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99095cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use tokenizer function to split text and add BPE to add in toekn ids\n",
    "# tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_gpt2_id, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac112ab4",
   "metadata": {},
   "source": [
    "Prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f589d514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paulpark/SandBox/CalendarProject/Notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9427e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('initial_prompt.yaml').resolve().parent.parent/\"Notebooks\"/\"initial_prompt.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9543e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, \"r\") as f:\n",
    "    initial_prompt = yaml.safe_load(f)[\"system_prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e5f2d",
   "metadata": {},
   "source": [
    "### gpt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0bb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "# convert the text to token IDs + attention mask. \n",
    "# and give tokens specific ids and return as pytorch tensor\n",
    "# encoded_input_gpt2 = tokenizer_gpt2(initial_prompt, return_tensors=\"pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f4b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained mdoel weights\n",
    "# download model weights into a PyTorch model object\n",
    "# model_gpt2 = AutoModelForCausalLM.from_pretrained(model_gpt2_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98996198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass (No Training)\n",
    "# forwards pass, running the input through the network layers once to compute output\n",
    "# inside the mode, it is doing:\n",
    "# lookup embeddings for each token id (token_ids -> Vectors)\n",
    "# run those vectors through all transformation layers (attention + MLP Blocks)\n",
    "# applies the LM head to produce logits (scores for next token)\n",
    "# out_gpt2 = model_gpt2(**encoded_input_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223dd6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the logits\n",
    "# Logits are the raw scores for what should be the next token\n",
    "# logits_gpt2 = out_gpt2.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738f31",
   "metadata": {},
   "source": [
    "Testing out the prompt, making sure prompt is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3f63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt1 = \"I have an interview tomorrow (jan 2026, 27th) at 12:30pm to 1pm. It's about CAPREIT basic interview phone call.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1a3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# converts human text to sequence of token ids\n",
    "# inputs_gpt2 = tokenizer_gpt2(prompt1, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c44ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling gradients\n",
    "# gradients are needed for training (updating weights)\n",
    "# for inference like generating text, keeping them wastes massive amounts of memory\n",
    "# with torch.no_grad():\n",
    "#     # this is the brain of the operation\n",
    "#     gen = model_gpt2.generate(\n",
    "#         **inputs_gpt2,     # takes input tokens\n",
    "#         max_new_tokens=200, # limits the response to 200 new tokens\n",
    "#         do_sample=False # enables greedy search, model will always pick the single most likely next word\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45e50510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding\n",
    "# converts the model's output back into human readable string\n",
    "# skip_special_tokens=True removes the special tokens like technical markers <|endoftext|>\n",
    "# print(tokenizer_gpt2.decode(gen[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259aa61",
   "metadata": {},
   "source": [
    "It reapeated my input thus, should do SFT \n",
    "\n",
    "check context window before input and after output \n",
    "\n",
    "Requires few shot prompting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d48a91",
   "metadata": {},
   "source": [
    "### lamma 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab9a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_llama3_mini = AutoTokenizer.from_pretrained(model_llama3_mini_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce0e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input_llama3 = tokenizer_llama3(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cc3806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_llama3 = AutoModelForCausalLM.from_pretrained(model_llama3_id, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90939f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_llama3 = model_llama3(**encoded_input_llama3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "949a07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_llama3 = out_llama3.logits  # vocab-size scores for next token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f3103",
   "metadata": {},
   "source": [
    "### Llama 3.2 1B instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd54cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_llama3_mini_id = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "591deec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tokenizer for llama3.1 mini\n",
    "tokenizer_llama3_mini = AutoTokenizer.from_pretrained(model_llama3_mini_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c45d613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained model and download the weights\n",
    "model_llama3_mini = AutoModelForCausalLM.from_pretrained(model_llama3_mini_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "863217fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": initial_prompt},\n",
    "  {\"role\": \"user\", \"content\": \"TODAY=2026-01-27\\nInterview for Company today from 10am to 11am. Interview is technical with basic python and ML knowledge.\"},\n",
    "  #{\"role\": \"assistant\", \"content\": \"{\\\"title\\\":\\\"Interview for Company\\\",\\\"event_date\\\":\\\"2026-01-27\\\",\\\"event_time\\\":\\\"10:00\\\",\\\"end_time\\\":\\\"11:00\\\",\\\"description\\\":\\\"Technical interview with basic Python and ML knowledge.\\\",\\\"notifications\\\":[],\\\"invitees\\\":[]}\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "704dbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readin input step\n",
    "inputs_llama3_mini = tokenizer_llama3_mini.apply_chat_template(\n",
    "    messages, # providing only system and user prompt\n",
    "    add_generation_prompt=True, # tells the model to start generating right after the last message (helps get a cleaner and more reliable response)\n",
    "    tokenize=True, # True returns tensors/tokenIDs, False returns the formatted string (better for debugging)\n",
    "    return_dict=True, # return tokenized inputs in dictionary format (helps with debugging)\n",
    "    return_tensors='pt' # return in pytorch tensors\n",
    ").to(model_llama3_mini.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bd02d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generation Step\n",
    "# .generate() will repeatedly in a loop to predict one token at a time\n",
    "# 1. forward pass -> logits\n",
    "# 2. choose next token based on decoding strategy\n",
    "# 3. append token to sequence\n",
    "# 4. repeat until it his a stop condition\n",
    "outputs_llama3_mini = model_llama3_mini.generate( \n",
    "                        **inputs_llama3_mini, # unpacks the dictionary  into keyword arguments\n",
    "                        max_new_tokens=400, # # how any tokens the model is allowed to add after the prompt\n",
    "                        do_sample=False, # turns of randomness, model uses greedy decoding (pick highest probability token each step)\n",
    "                        temperature=0.0, # This field is ignored, since do_sample=False. If do_sample=True, this controls the randomness of the sampling process.\n",
    "                        output_scores=True, # instructs the model to store and return the raw logits (scores for every word)\n",
    "                        return_dict_in_generate=True, # returns a dictionary instead of a list of tensors of the outputs\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513b052",
   "metadata": {},
   "source": [
    "check how many tokens were used in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "628f57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_len = inputs_llama3_mini[\"input_ids\"].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965fa5a",
   "metadata": {},
   "source": [
    "check how many new tokens were generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9210d75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_llama3_mini.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "390c1c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_llama3_mini.sequences.shape[-1] - prompt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38c595cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated part only\n",
    "print(tokenizer_llama3_mini.decode(outputs_llama3_mini[0][inputs_llama3_mini['input_ids'].shape[-1]:],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8dd111",
   "metadata": {},
   "source": [
    "We need to do a simple evaluation on how confident the model is in its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67fd2b9",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370de83",
   "metadata": {},
   "source": [
    "## Confidence /Behavior (model-level)\n",
    "\n",
    "This metric tellls you how confident the model is in its predictions not the accuracy of the predictions.\n",
    "\n",
    "Confidence metrics are used to diagnose the model, and will help me understand if SFT has improved model confidence or not.\n",
    "\n",
    "- Logits (token-level)\n",
    "- Mean Token Probability\n",
    "- Mean Entropy\n",
    "- Length Normalized Log Likelihood\n",
    "\n",
    "## Accuracy / Task Success\n",
    "\n",
    "This metric tellls you how accurate the model is in its predictions not the confidence of the predictions.\n",
    "\n",
    "- JSON Validity Rate\n",
    "- Schema Accuracy\n",
    "- Exact Match\n",
    "- Slot F1\n",
    "\n",
    "## Evaluation Methods\n",
    "\n",
    "- Log Likelihood / Perplexity \n",
    "- Token Probabilities (can be calculated from logit outputs using softmax)\n",
    "- Entropy (Measures uncertainty of model, High entropy = less confidence and tends to hallucinate, low entropy = more confidence)\n",
    "\n",
    "- Mean token Probability -> average probability of outputted tokens, measures confidence, higher the better.\n",
    "- Mean entropy -> Measures the mean uncertainty of the model, lower the better.\n",
    "- Length Normalized Log Likelihood -> log likelihood gets worse the longer the sentence, normalizing it would help compare model's confidence across different lengths of outputs.\n",
    "\n",
    "## Task- specific Automatic metrics\n",
    "\n",
    "- exact match\n",
    "- F1 / Precision / Recall\n",
    "- Structured output accuracy\n",
    "\n",
    "## Human Evaluation (Final step for Chat, assistant, sugjective outputs)\n",
    "\n",
    "- Human Preference ranking (RL)\n",
    "- Helpfulness / corectness judgements\n",
    "- Pairwise comparisons\n",
    "\n",
    "### Using LOGITS (Model's Confidence)\n",
    "\n",
    "### Using ENTROPY (measuring Uncertaingy)\n",
    "\n",
    "### Using Mean Token Probability\n",
    "\n",
    "### Mean Entropy\n",
    "\n",
    "### Mean Length Normalized Log Likelihood\n",
    "\n",
    "### Schema Accuracy\n",
    "\n",
    "### Exact Match\n",
    "\n",
    "### Slot F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec73b62",
   "metadata": {},
   "source": [
    "## Model confidence\n",
    "For now, we will focus on Mean Entropy and mean Log Likelihood for the model confidence side. Purpose is to get a quick metric to understand the model confidence.\n",
    "\n",
    "Mean Log-Likelihood -> how probable were the tokens the model chose? \n",
    "\n",
    "Mean entropy -> how certain was the model while generating?\n",
    "\n",
    "## Model Accuracy\n",
    "For the model accuracy side, we will focus on Precision, Recall, and F1 Score.\n",
    "\n",
    "Precision at Slot level: \"of allthe times the model predicted Structured, how many were actually structured?\"\n",
    "\n",
    "Recall at Slot level: \"Of all the structured examples that exist, how many did the model actually find?\"\n",
    "\n",
    "F1 Score at Slot level: \"A balance between Precision and Recall, often used as the \"true\" indicator of accuracy\"\n",
    "\n",
    "JSON Validity Rate: percentage of outputs that can be successfully parsed as valid JSON. This is binary metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db52b3",
   "metadata": {},
   "source": [
    "# Actual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "784505a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/paulpark/SandBox/CalendarProject/Notebooks'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7235ae4",
   "metadata": {},
   "source": [
    "loading test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "de3618ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, dict_keys(['input', 'output']))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data files\n",
    "with open(\"./sample_data_gemini.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cases_gemini = json.load(f)\n",
    "len(cases_gemini), cases_gemini[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9a49676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, dict_keys(['input', 'output']))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./sample_data_gpt.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cases_gpt = json.load(f)\n",
    "len(cases_gpt), cases_gpt[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0ba8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cases = cases_gemini + cases_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc901ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a815fb",
   "metadata": {},
   "source": [
    "loading tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_llama3_mini\n",
    "model_llama3_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rows =[]\n",
    "for i, ex in enumerate(full_cases,start=1): # the loop takes the list of dictionary \"full_cases\", start counting from 1 instead of 0\n",
    "    user_text = ex[\"input\"] # read the input\n",
    "    \n",
    "    # tokenize the NLP input to embeddings model can understand\n",
    "    inputs = tokenizer_llama3_mini.apply_chat_template(\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db987c23",
   "metadata": {},
   "source": [
    "# Mean Length normalized Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701c1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aae8a2b5",
   "metadata": {},
   "source": [
    "# Mean Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74b4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd210a8c",
   "metadata": {},
   "source": [
    "this output looks alot better\n",
    "\n",
    "Now lets start the SFT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086c9b6",
   "metadata": {},
   "source": [
    "# SFT (Supervised Fine-Tuning) for synthetic Dataset\n",
    "\n",
    "## Pretraining\n",
    "\n",
    "- The model learns general language from internet\n",
    "\n",
    "## Supervised Fine-Tuning\n",
    "\n",
    "- Taking the base model and train it on few thousands examples of NLP text -> JSON pairs\n",
    "\n",
    "## Preference Tuning (RLHF/DOP)\n",
    "\n",
    "- After the model learns the JSON format (SFT), you use preference tuning to align it, teaching it to prefer valid, clean JSON over messry or conversational output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008e946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed1e3a46",
   "metadata": {},
   "source": [
    "# Evaluate LLM\n",
    "\n",
    "### JSON validity rate, Field level accuracy, business metric % events created without manual edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddb32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6ea1c2",
   "metadata": {},
   "source": [
    "# DPO\n",
    "### Generate Pairs (good parse vs bad parse) from your synthetic edge cases, Train with TRL's DPO style pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandboxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
